{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving a constrained Integer Linear Program using learned cost\n",
    "\n",
    "We consider the problem of minimizing a constrained ILP program. The problem can be written formally as\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathbf{x}^{\\ast} =& \\mathop{\\arg\\min}_{\\mathbf{x} \\in \\mathcal{X}} \\mathbf{c(f;\\mathbf{w})}^T \\mathbf{x} \\\\\n",
    "&\\text{s.t.} \\begin{aligned}[t]\n",
    "     \\mathbf{A}\\mathbf{x} & = \\mathbf{b} \\\\\n",
    "     \\mathbf{G}\\mathbf{x} & \\leq \\mathbf{h}\n",
    "  \\end{aligned}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where $\\mathbf{c(f;\\mathbf{w})} \\in \\mathbb{R}^n$ is the cost function parametrized by $\\mathbf{\\mathbf{w}}$, given input $\\mathbf{f}$. And $\\mathbf{A,b}$ and $\\mathbf{G,h}$ defines the equality and in-equality constraints, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, time\n",
    "import cv2, random\n",
    "import pickle, joblib\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import gurobipy as gp\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from lib.tracking import Tracker\n",
    "from lib.utils import getIoU, computeBoxFeatures, interpolateTrack, interpolateTracks\n",
    "\n",
    "class Net(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(6,6), nn.ReLU(), nn.Linear(6,1))\n",
    "    def forward(self, data):\n",
    "        x = self.fc(data.edge_attr)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "net.load_state_dict(torch.load('ckpt/visdrone/epoch-6.pth'))\n",
    "#net.load_state_dict(torch.load('ckpt/qp/epoch_11.pth'))\n",
    "\n",
    "#net.load_state_dict(torch.load('../ckpt/bce/epoch_0010.pth'))\n",
    "#net.load_state_dict(torch.load('../ckpt/spo_mlp/epoch_15.pth'))\n",
    "#net.load_state_dict(torch.load('../ckpt/qptl_l2/epoch_0009.pth'))\n",
    "#net.load_state_dict(torch.load('../ckpt/qptl_l1/epoch_0009.pth'))\n",
    "\n",
    "tracker = Tracker(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trans_probs(tracker, curr_dets, curr_app_feats, app_thresh, max_frame_gap = 5):\n",
    "    \"\"\"\n",
    "    Inputs: tracker: an instance of the Tracker.\n",
    "            curr_dets: frame, x1, y1, x2, y2, det_confidence, node_ind.\n",
    "            curr_app_feats: normalized appearance features for curr_dets.\n",
    "            max_frame_gap: frame gap used to connect detections.\n",
    "    Return: transition probabilities for LP that handles false negatives(missing detections).\n",
    "    \"\"\"\n",
    "    edge_ind = 0\n",
    "    edge_feats, lifted_probs = [], []\n",
    "    edge_type = [] #1:base edge 2:lifted edge-1:pruned lifted edge\n",
    "    \n",
    "    cos_sim_mat = np.dot(curr_app_feats, curr_app_feats.T)\n",
    "    linkIndexGraph = np.zeros((curr_dets.shape[0], curr_dets.shape[0]), dtype=np.int32)\n",
    "    for i in range(curr_dets.shape[0]):\n",
    "        for j in range(curr_dets.shape[0]):\n",
    "            frame_gap = curr_dets[j][0] - curr_dets[i][0]\n",
    "            cos_sim = cos_sim_mat[i, j]\n",
    "\n",
    "            if frame_gap == 1: #base edge\n",
    "                edge_type.append(1)\n",
    "                feats = computeBoxFeatures(curr_dets[i, 1:5], curr_dets[j, 1:5])\n",
    "                iou = getIoU(curr_dets[i, 1:5], curr_dets[j, 1:5])\n",
    "                feats.extend((iou, cos_sim))\n",
    "                edge_feats.append(feats)\n",
    "                edge_ind += 1\n",
    "                linkIndexGraph[i, j] = edge_ind\n",
    "\n",
    "            elif frame_gap > 1 and frame_gap <= max_frame_gap: #lifted edge\n",
    "                if cos_sim > app_thresh:\n",
    "                    edge_type.append(2)\n",
    "                    time_weight = 0.9 ** frame_gap\n",
    "                    lifted_probs.append(cos_sim * time_weight)\n",
    "                else:\n",
    "                    edge_type.append(-1)\n",
    "\n",
    "                edge_ind += 1\n",
    "                linkIndexGraph[i, j] = edge_ind\n",
    "                \n",
    "    edge_type = np.array(edge_type)\n",
    "    edge_feats = torch.Tensor(edge_feats)\n",
    "    with torch.no_grad():\n",
    "        logits = tracker.net.fc(edge_feats)\n",
    "        prob = nn.Sigmoid()(logits)\n",
    "        prob = torch.clamp(prob, min=1e-7, max=1-1e-7).flatten().numpy()\n",
    "        \n",
    "    probs = np.zeros(edge_ind)\n",
    "    base_inds = np.where(edge_type == 1)[0]\n",
    "    lifted_inds = np.where(edge_type == 2)[0]\n",
    "    pruned_lifted_inds = np.where(edge_type == -1)[0]\n",
    "    probs[base_inds] = prob            #base probs\n",
    "    probs[lifted_inds] = lifted_probs  #lifted probs\n",
    "    return linkIndexGraph, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence uav0000009_03358_v, DPM detection, app thresh 0.75, dist thresh 100, retain length 3\n",
      "(13256, 10) (13256, 2048)\n",
      "Tracking from frame 1 to 100\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 96 to 195\n",
      "0-th iteration\n"
     ]
    }
   ],
   "source": [
    "app_thresh = 0.75 #0.7, 0.8\n",
    "nms_thresh, eps = 0.3, 1e-7\n",
    "\n",
    "# for seq in ['MOT17-01', 'MOT17-03', 'MOT17-06', 'MOT17-07', 'MOT17-08', 'MOT17-12', 'MOT17-14']:\n",
    "# for seq in ['uav0000086_00000_v', 'uav0000117_02622_v', 'uav0000137_00458_v', 'uav0000182_00000_v', 'uav0000268_05773_v', 'uav0000305_00000_v', 'uav0000339_00001_v']:\n",
    "\n",
    "for seq in ['uav0000009_03358_v', 'uav0000073_00600_v', 'uav0000073_04464_v', 'uav0000077_00720_v', 'uav0000088_00290_v', 'uav0000119_02301_v', 'uav0000120_04775_v', 'uav0000161_00000_v', 'uav0000188_00000_v', 'uav0000201_00000_v', 'uav0000249_00001_v', 'uav0000249_02688_v', 'uav0000297_00000_v', 'uav0000297_02761_v', 'uav0000306_00230_v', 'uav0000355_00001_v', 'uav0000370_00001_v']:\n",
    "    img = Image.open(f\"/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test-dev/sequences/{seq}/0000001.jpg\").convert('RGB')\n",
    "    # img =  Image.open(f\"/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-val/sequences/{seq}/0000001.jpg\").convert('RGB')\n",
    "    img_Height, img_Width = img.size\n",
    "\n",
    "    #Static camera, moving camera\n",
    "    if seq in ['MOT17-03']:\n",
    "        batch_size, dist_thresh, prune_len = 50, 50, 2 #tracklets less than 2 are pruned\n",
    "    else:\n",
    "        batch_size, dist_thresh, prune_len = 100, 100, 3\n",
    "        \n",
    "    # if seq == 'MOT17-06':\n",
    "    #     img_Height, img_Width = 480, 640\n",
    "    # else:\n",
    "    #     img_Height, img_Width = 1080, 1920\n",
    "        \n",
    "    #for detector in ['DPM','FRCNN','SDP']:\n",
    "    for detector in ['DPM']:\n",
    "        print('Sequence {}, {} detection, app thresh {}, dist thresh {}, retain length {}'.format(\n",
    "            seq, detector, app_thresh, dist_thresh, prune_len))\n",
    "        det_file = f\"VisDrone/VisDrone2019-MOT-test-dev/det/{seq}.txt\"\n",
    "        app_file = f\"VisDrone/VisDrone2019-MOT-test-dev/feature/{seq}\"\n",
    "\n",
    "        # det_file = f\"VisDrone/VisDrone2019-MOT-val/annotations/{seq}.txt\"\n",
    "        # app_file = f\"VisDrone/VisDrone2019-MOT-val/feature/{seq}\"\n",
    "\n",
    "        # det_file = './result/{}/det_{}.txt'.format(seq, detector)\n",
    "        # app_file = './result/{}/app_det_{}.npy'.format(seq, detector)\n",
    "\n",
    "        dets = np.loadtxt(det_file, delimiter=',')\n",
    "        app_feats = np.load(app_file)\n",
    "        print(dets.shape, app_feats.shape)\n",
    "        assert dets.shape[0] == app_feats.shape[0], 'Shape mismatch'\n",
    "\n",
    "        batch_overlap = 5                  #Number of frames to overlap between 2 batches\n",
    "        num_frames = int(dets[:, 0].max()) #Number of frames for this video\n",
    "        tracks_list, assignments_list, features_list, nms_list = [],[],[],[]\n",
    "        \n",
    "        for start_frame in range(1, num_frames+1, batch_size-batch_overlap):\n",
    "            end_frame = start_frame + batch_size - 1\n",
    "            if end_frame >= num_frames:\n",
    "                end_frame = num_frames\n",
    "                \n",
    "            print('Tracking from frame %d to %d'%(start_frame, end_frame))\n",
    "            curr_ind = np.logical_and(dets[:, 0] >= start_frame, dets[:, 0] <= end_frame)\n",
    "            curr_dets = np.concatenate([dets[curr_ind, 0][:, None], dets[curr_ind, 2:7],\n",
    "                                        np.arange(dets[curr_ind].shape[0])[:, None]], axis=1)\n",
    "\n",
    "            curr_dets[:, 3:5] = curr_dets[:, 3:5] + curr_dets[:, 1:3] # convert to frame,x1,y1,x2,y2,conf,node_ind\n",
    "            curr_app_feats = app_feats[curr_ind]\n",
    "            curr_app_feats = curr_app_feats / np.linalg.norm(curr_app_feats, axis=1, keepdims=True)\n",
    "            for iteration in range(2):\n",
    "                if iteration == 0:\n",
    "                    print('%d-th iteration'%iteration)\n",
    "                    linkIndexGraph, probs = get_trans_probs(tracker, curr_dets, curr_app_feats, \n",
    "                                                            app_thresh, max_frame_gap = 5)\n",
    "                    trans_cost = - np.log(probs + eps) #np.log((1 - probs + eps)/(probs + eps))\n",
    "                    det_cost = - curr_dets[:, -2]\n",
    "                    entry_cost = 0.5 * np.ones(det_cost.shape[0])\n",
    "                    exit_cost = entry_cost\n",
    "                    cost = np.concatenate((det_cost, entry_cost, exit_cost, trans_cost))\n",
    "\n",
    "                    A_eq, b_eq, A_ub, b_ub = tracker.build_constraint(linkIndexGraph)\n",
    "                    sol = tracker.linprog(c=cost, A_eq=A_eq, b_eq=b_eq, A_ub=A_ub, b_ub=b_ub)\n",
    "                    \n",
    "                    tracklets = tracker.recoverTracklets(curr_dets, sol, linkIndexGraph, prune_len=prune_len)    \n",
    "                    tracklets_ = np.delete(tracklets, -1, axis=1)\n",
    "                    interpolated_tracklets = interpolateTracks(tracklets_)\n",
    "                    \n",
    "                else:\n",
    "                    print('%d-th iteration'%iteration)\n",
    "                    assignment_list, feature_list = tracker.clusterSkipTracklets(tracklets, curr_app_feats, \n",
    "                                                                                 dist_thresh, app_thresh)\n",
    "                    tracks = tracker.recoverClusteredTracklets(tracklets, assignment_list)\n",
    "                    tracks = interpolateTracks(tracks)\n",
    "\n",
    "                    assignments_list.append(assignment_list)\n",
    "                    feature_array = np.stack(feature_list)\n",
    "                    feature_array = feature_array / np.linalg.norm(feature_array, axis=1, keepdims=True)\n",
    "                    \n",
    "            tracks_list.append(tracks)\n",
    "            features_list.append(feature_array)\n",
    "            \n",
    "        final_tracks = tracker.stitchTracklets(tracks_list, features_list)\n",
    "        save_file = 'output_visdrone/{}-{}.txt'.format(seq, detector)\n",
    "        print('Finished tracking, saving to {}'.format(save_file))\n",
    "        np.savetxt(save_file, final_tracks, fmt='%d',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show final tracking and detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uav0000009_03358_v\n",
      "save dir output_visdrone/uav0000009_03358_v-GT\n",
      "Processing frame 100\n",
      "Processing frame 200\n",
      "Video saved at output_visdrone/uav0000009_03358_v-GT/uav0000009_03358_v-GT.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#seq = 'MOT17-03'\n",
    "detector = 'GT'\n",
    "# ['uav0000013_00000_v', 'uav0000013_01073_v', 'uav0000013_01392_v', 'uav0000020_00406_v', 'uav0000071_03240_v', 'uav0000072_04488_v']\n",
    "# for seq in ['MOT17-01', 'MOT17-03', 'MOT17-06', 'MOT17-07', 'MOT17-08', 'MOT17-12', 'MOT17-14']:\n",
    "# for seq in os.listdir(\"/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-val/sequences/\"):\n",
    "for seq in ['uav0000009_03358_v']:\n",
    "    print(seq)\n",
    "    save_dir = 'output_visdrone/{}-{}'.format(seq, detector)\n",
    "    print('save dir {}'.format(save_dir))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    #tracks: frame, ID, x, y, w, h, -1, -1, -1, -1\n",
    "    #dets:   frame, -1, x, y, w, h, conf, -1, -1, -1\n",
    "    # tracks = np.loadtxt(f'BYTE_Results/{seq}-{detector}.txt', delimiter=',')\n",
    "    # tracks = np.loadtxt(f'output_visdrone/{seq}-{detector}.txt', delimiter=',')\n",
    "    tracks = np.loadtxt(f'VisDrone/VisDrone2019-MOT-test-dev/det/{seq}.txt', delimiter=',')\n",
    "    \n",
    "    tracks = tracks.astype(np.int32)\n",
    "    \n",
    "    colors = np.random.rand(1000,3)\n",
    "    resize_scale = 0.5\n",
    "    \n",
    "    # Get the first frame to determine video dimensions\n",
    "    first_frame = tracks[:, 0].min()\n",
    "    # img_file = os.path.join('data/MOT/MOT17/test/{}-{}/img1/{:06d}.jpg'.format(seq, detector, first_frame))\n",
    "    img_file = os.path.join('/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test-dev/sequences/{}/{:07d}.jpg').format(seq, first_frame)\n",
    "    img = cv2.imread(img_file)\n",
    "    height, width = int(resize_scale*img.shape[0]), int(resize_scale*img.shape[1])\n",
    "    \n",
    "    # Initialize video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # MP4 codec\n",
    "    video_path = os.path.join(save_dir, '{}-{}.mp4'.format(seq, detector))\n",
    "    fps = 30  # Frames per second for the output video\n",
    "    video_writer = cv2.VideoWriter(video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    for frame in range(tracks[:, 0].min(), tracks[:, 0].max()+1):\n",
    "        if frame % 100 == 0:\n",
    "            print('Processing frame {}'.format(frame))\n",
    "        \n",
    "        img_file = os.path.join('/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test-dev/sequences/{}/{:07d}.jpg').format(seq, frame)\n",
    "        img = cv2.imread(img_file)\n",
    "        img = cv2.resize(img, (int(resize_scale*img.shape[1]), int(resize_scale*img.shape[0])))\n",
    "        cv2.putText(img, '{:04}'.format(frame), (0,50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,0,255), thickness=2)\n",
    "        bboxes = tracks[tracks[:, 0] == frame, 1:6]\n",
    "        \n",
    "        if bboxes.shape[0] != 0:\n",
    "            #detections = dets[dets[:, 0] == frame, 2:7]\n",
    "            for i in range(bboxes.shape[0]):\n",
    "                ID = int(bboxes[i][0])\n",
    "                x, y = int(resize_scale*(bboxes[i][1])), int(resize_scale*(bboxes[i][2]))\n",
    "                w, h = int(resize_scale*(bboxes[i][3])), int(resize_scale*(bboxes[i][4]))\n",
    "                cv2.rectangle(img, (x,y), (x+w,y+h), 255*colors[ID], thickness=2)\n",
    "                cv2.putText(img, str(ID), (x,y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, 255*colors[ID], thickness=2)\n",
    "        \n",
    "        # Write frame to both image file and video\n",
    "        cv2.imwrite(save_dir+'/'+'{:06d}.jpg'.format(frame), img)\n",
    "        video_writer.write(img)\n",
    "    \n",
    "    # Release the video writer\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved at {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"BYTE_Results/MOT17-01-DPM/MOT17-01-DPM.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"BYTE_Results/MOT17-01-DPM/MOT17-01-DPM.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import generalized_box_iou\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khanh/miniconda3/envs/LPT/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/khanh/miniconda3/envs/LPT/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Load pretrained ResNet50 model without the final classification layer\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "# 2. Define image transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 3. Custom dataset to apply transforms\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, transform):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.images[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data_name in tqdm(os.listdir(\"/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test/sequences/\")):\n",
    "for data_name in ['uav0000009_03358_v', 'uav0000073_00600_v', 'uav0000073_04464_v', 'uav0000077_00720_v', 'uav0000088_00290_v', 'uav0000119_02301_v', 'uav0000120_04775_v', 'uav0000161_00000_v', 'uav0000188_00000_v', 'uav0000201_00000_v', 'uav0000249_00001_v', 'uav0000249_02688_v', 'uav0000297_00000_v', 'uav0000297_02761_v', 'uav0000306_00230_v', 'uav0000355_00001_v', 'uav0000370_00001_v']:\n",
    "    # label_path = f\"/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test-dev/annotations/{data_name}.txt\"\n",
    "    label_path = f\"/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test-dev/det/{data_name}.txt\"\n",
    "    video_path = f\"/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test-dev/sequences/{data_name}/\"\n",
    "    frame_names = os.listdir(video_path)\n",
    "    frame_names.sort()\n",
    "    frame_path_list = [os.path.join(video_path, frame_names[i]) for i in range(len(frame_names))]\n",
    "    # <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <x>, <y>, <z>\n",
    "    gt = []\n",
    "    with open(label_path, 'r') as f:\n",
    "        for l in f.readlines():\n",
    "            l = l.strip().split(',')\n",
    "            gt.append([int(i) for i in l[:-1]] + [float(l[-1])])\n",
    "    gt = np.array(gt)\n",
    "    gt[:,4:6] = gt[:,2:4] + gt[:,4:6]\n",
    "    \n",
    "    video_frames = [None]\n",
    "    for path in frame_path_list:\n",
    "        video_frames.append(Image.open(path).convert('RGB'))\n",
    "    \n",
    "    isinstance_PIL = []\n",
    "    for instance in gt:\n",
    "        isinstance_PIL.append(video_frames[int(instance[0])].crop(instance[2:6]))\n",
    "    \n",
    "    instance_embedding = []\n",
    "    dataset = ImageDataset(isinstance_PIL, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            output = resnet(batch).squeeze(-1).squeeze(-1)  # Shape: (B, 2048)\n",
    "            embeddings = output.cpu()  # Move to CPU if needed\n",
    "            instance_embedding.extend(embeddings)\n",
    "    instance_embedding = torch.stack(instance_embedding)\n",
    "    \n",
    "    with open(f'/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test-dev/feature/{data_name}', 'wb') as f:\n",
    "        np.save(f, instance_embedding.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uav0000009_03358_v', 'uav0000073_00600_v', 'uav0000073_04464_v', 'uav0000077_00720_v', 'uav0000088_00290_v', 'uav0000119_02301_v', 'uav0000120_04775_v', 'uav0000161_00000_v', 'uav0000188_00000_v', 'uav0000201_00000_v', 'uav0000249_00001_v', 'uav0000249_02688_v', 'uav0000297_00000_v', 'uav0000297_02761_v', 'uav0000306_00230_v', 'uav0000355_00001_v', 'uav0000370_00001_v']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test-dev/sequences/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LPT",
   "language": "python",
   "name": "lpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
