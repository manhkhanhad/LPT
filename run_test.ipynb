{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving a constrained Integer Linear Program using learned cost\n",
    "\n",
    "We consider the problem of minimizing a constrained ILP program. The problem can be written formally as\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathbf{x}^{\\ast} =& \\mathop{\\arg\\min}_{\\mathbf{x} \\in \\mathcal{X}} \\mathbf{c(f;\\mathbf{w})}^T \\mathbf{x} \\\\\n",
    "&\\text{s.t.} \\begin{aligned}[t]\n",
    "     \\mathbf{A}\\mathbf{x} & = \\mathbf{b} \\\\\n",
    "     \\mathbf{G}\\mathbf{x} & \\leq \\mathbf{h}\n",
    "  \\end{aligned}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where $\\mathbf{c(f;\\mathbf{w})} \\in \\mathbb{R}^n$ is the cost function parametrized by $\\mathbf{\\mathbf{w}}$, given input $\\mathbf{f}$. And $\\mathbf{A,b}$ and $\\mathbf{G,h}$ defines the equality and in-equality constraints, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3077952/1201081467.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load('ckpt/visdrone/epoch-20.pth'))\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, time\n",
    "import cv2, random\n",
    "import pickle, joblib\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import gurobipy as gp\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lib.tracking import Tracker\n",
    "from lib.utils import getIoU, computeBoxFeatures, interpolateTrack, interpolateTracks\n",
    "\n",
    "class Net(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(6,6), nn.ReLU(), nn.Linear(6,1))\n",
    "    def forward(self, data):\n",
    "        x = self.fc(data.edge_attr)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "net.load_state_dict(torch.load('ckpt/visdrone/epoch-20.pth'))\n",
    "#net.load_state_dict(torch.load('ckpt/qp/epoch_11.pth'))\n",
    "\n",
    "#net.load_state_dict(torch.load('../ckpt/bce/epoch_0010.pth'))\n",
    "#net.load_state_dict(torch.load('../ckpt/spo_mlp/epoch_15.pth'))\n",
    "#net.load_state_dict(torch.load('../ckpt/qptl_l2/epoch_0009.pth'))\n",
    "#net.load_state_dict(torch.load('../ckpt/qptl_l1/epoch_0009.pth'))\n",
    "\n",
    "tracker = Tracker(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trans_probs(tracker, curr_dets, curr_app_feats, app_thresh, max_frame_gap = 5):\n",
    "    \"\"\"\n",
    "    Inputs: tracker: an instance of the Tracker.\n",
    "            curr_dets: frame, x1, y1, x2, y2, det_confidence, node_ind.\n",
    "            curr_app_feats: normalized appearance features for curr_dets.\n",
    "            max_frame_gap: frame gap used to connect detections.\n",
    "    Return: transition probabilities for LP that handles false negatives(missing detections).\n",
    "    \"\"\"\n",
    "    edge_ind = 0\n",
    "    edge_feats, lifted_probs = [], []\n",
    "    edge_type = [] #1:base edge 2:lifted edge-1:pruned lifted edge\n",
    "    \n",
    "    cos_sim_mat = np.dot(curr_app_feats, curr_app_feats.T)\n",
    "    linkIndexGraph = np.zeros((curr_dets.shape[0], curr_dets.shape[0]), dtype=np.int32)\n",
    "    for i in range(curr_dets.shape[0]):\n",
    "        for j in range(curr_dets.shape[0]):\n",
    "            frame_gap = curr_dets[j][0] - curr_dets[i][0]\n",
    "            cos_sim = cos_sim_mat[i, j]\n",
    "\n",
    "            if frame_gap == 1: #base edge\n",
    "                edge_type.append(1)\n",
    "                feats = computeBoxFeatures(curr_dets[i, 1:5], curr_dets[j, 1:5])\n",
    "                iou = getIoU(curr_dets[i, 1:5], curr_dets[j, 1:5])\n",
    "                feats.extend((iou, cos_sim))\n",
    "                edge_feats.append(feats)\n",
    "                edge_ind += 1\n",
    "                linkIndexGraph[i, j] = edge_ind\n",
    "\n",
    "            elif frame_gap > 1 and frame_gap <= max_frame_gap: #lifted edge\n",
    "                if cos_sim > app_thresh:\n",
    "                    edge_type.append(2)\n",
    "                    time_weight = 0.9 ** frame_gap\n",
    "                    lifted_probs.append(cos_sim * time_weight)\n",
    "                else:\n",
    "                    edge_type.append(-1)\n",
    "\n",
    "                edge_ind += 1\n",
    "                linkIndexGraph[i, j] = edge_ind\n",
    "                \n",
    "    edge_type = np.array(edge_type)\n",
    "    edge_feats = torch.Tensor(edge_feats)\n",
    "    with torch.no_grad():\n",
    "        logits = tracker.net.fc(edge_feats)\n",
    "        prob = nn.Sigmoid()(logits)\n",
    "        prob = torch.clamp(prob, min=1e-7, max=1-1e-7).flatten().numpy()\n",
    "        \n",
    "    probs = np.zeros(edge_ind)\n",
    "    base_inds = np.where(edge_type == 1)[0]\n",
    "    lifted_inds = np.where(edge_type == 2)[0]\n",
    "    pruned_lifted_inds = np.where(edge_type == -1)[0]\n",
    "    probs[base_inds] = prob            #base probs\n",
    "    probs[lifted_inds] = lifted_probs  #lifted probs\n",
    "    return linkIndexGraph, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(data):\n",
    "    # Extract frame_id and object_id columns and make them a structured array for uniqueness\n",
    "    keys = data[:, :2].astype(np.int64)  # assuming frame_id and object_id are integers\n",
    "    # Convert keys to a string or tuple that can be hashed\n",
    "    _, idx = np.unique(keys, axis=0, return_index=True)\n",
    "    # Sort indices to keep original order\n",
    "    idx_sorted = np.sort(idx)\n",
    "    return data[idx_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = [\"uav0000009_03358_v\",\n",
    "\"uav0000073_00600_v\",\n",
    "\"uav0000077_00720_v\",\n",
    "\"uav0000088_00290_v\",\n",
    "\"uav0000119_02301_v\",\n",
    "\"uav0000120_04775_v\",\n",
    "\"uav0000188_00000_v\",\n",
    "\"uav0000201_00000_v\",\n",
    "\"uav0000249_00001_v\",\n",
    "\"uav0000249_02688_v\",\n",
    "\"uav0000297_00000_v\",\n",
    "\"uav0000297_02761_v\",\n",
    "\"uav0000306_00230_v\",\n",
    "\"uav0000355_00001_v\",\n",
    "\"uav0000370_00001_v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence uav0000009_03358_v, GT detection, app thresh 0.75, dist thresh 100, retain length 3\n",
      "(12740, 10) (12740, 2048)\n",
      "Tracking from frame 1 to 100\n",
      "0-th iteration\n",
      "Set parameter Username\n",
      "Set parameter LicenseID to value 2641140\n",
      "Academic license - for non-commercial use only - expires 2026-03-24\n",
      "1-th iteration\n",
      "Tracking from frame 96 to 195\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 191 to 219\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Finished tracking, saving to output_visdrone_v2_e20/uav0000009_03358_v-GT.txt\n",
      "Sequence uav0000073_00600_v, GT detection, app thresh 0.75, dist thresh 100, retain length 3\n",
      "(14721, 10) (14721, 2048)\n",
      "Tracking from frame 1 to 100\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 96 to 195\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 191 to 290\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 286 to 328\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Finished tracking, saving to output_visdrone_v2_e20/uav0000073_00600_v-GT.txt\n",
      "Sequence uav0000077_00720_v, GT detection, app thresh 0.75, dist thresh 100, retain length 3\n",
      "(19244, 10) (19244, 2048)\n",
      "Tracking from frame 1 to 100\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 96 to 195\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 191 to 290\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 286 to 385\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 381 to 480\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 476 to 575\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 571 to 670\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 666 to 765\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 761 to 780\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Finished tracking, saving to output_visdrone_v2_e20/uav0000077_00720_v-GT.txt\n",
      "Sequence uav0000088_00290_v, GT detection, app thresh 0.75, dist thresh 100, retain length 3\n",
      "(24315, 10) (24315, 2048)\n",
      "Tracking from frame 1 to 100\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 96 to 195\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 191 to 290\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 286 to 296\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Finished tracking, saving to output_visdrone_v2_e20/uav0000088_00290_v-GT.txt\n",
      "Sequence uav0000119_02301_v, GT detection, app thresh 0.75, dist thresh 100, retain length 3\n",
      "(7208, 10) (7208, 2048)\n",
      "Tracking from frame 1 to 100\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 96 to 179\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Finished tracking, saving to output_visdrone_v2_e20/uav0000119_02301_v-GT.txt\n",
      "Sequence uav0000120_04775_v, GT detection, app thresh 0.75, dist thresh 100, retain length 3\n",
      "(45881, 10) (45881, 2048)\n",
      "Tracking from frame 1 to 100\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 96 to 195\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 191 to 290\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 286 to 385\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 381 to 480\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 476 to 575\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 571 to 670\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 666 to 765\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 761 to 860\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 856 to 955\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 951 to 1000\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Finished tracking, saving to output_visdrone_v2_e20/uav0000120_04775_v-GT.txt\n",
      "Sequence uav0000188_00000_v, GT detection, app thresh 0.75, dist thresh 100, retain length 3\n",
      "(21011, 10) (21011, 2048)\n",
      "Tracking from frame 1 to 100\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 96 to 195\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 191 to 260\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Finished tracking, saving to output_visdrone_v2_e20/uav0000188_00000_v-GT.txt\n",
      "Sequence uav0000201_00000_v, GT detection, app thresh 0.75, dist thresh 100, retain length 3\n",
      "(21215, 10) (21215, 2048)\n",
      "Tracking from frame 1 to 100\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 96 to 195\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 191 to 290\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 286 to 385\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 381 to 480\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 476 to 575\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 571 to 670\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 666 to 677\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Finished tracking, saving to output_visdrone_v2_e20/uav0000201_00000_v-GT.txt\n",
      "Sequence uav0000249_00001_v, GT detection, app thresh 0.75, dist thresh 100, retain length 3\n",
      "(9367, 10) (9367, 2048)\n",
      "Tracking from frame 1 to 100\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 96 to 195\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 191 to 290\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 286 to 360\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Finished tracking, saving to output_visdrone_v2_e20/uav0000249_00001_v-GT.txt\n",
      "Sequence uav0000249_02688_v, GT detection, app thresh 0.75, dist thresh 100, retain length 3\n",
      "(8037, 10) (8037, 2048)\n",
      "Tracking from frame 1 to 100\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 96 to 195\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 191 to 244\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Finished tracking, saving to output_visdrone_v2_e20/uav0000249_02688_v-GT.txt\n",
      "Sequence uav0000297_00000_v, GT detection, app thresh 0.75, dist thresh 100, retain length 3\n",
      "(10297, 10) (10297, 2048)\n",
      "Tracking from frame 1 to 100\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 96 to 146\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Finished tracking, saving to output_visdrone_v2_e20/uav0000297_00000_v-GT.txt\n",
      "Sequence uav0000297_02761_v, GT detection, app thresh 0.75, dist thresh 100, retain length 3\n",
      "(31647, 10) (31647, 2048)\n",
      "Tracking from frame 1 to 100\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 96 to 195\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 191 to 290\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 286 to 373\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Finished tracking, saving to output_visdrone_v2_e20/uav0000297_02761_v-GT.txt\n",
      "Sequence uav0000306_00230_v, GT detection, app thresh 0.75, dist thresh 100, retain length 3\n",
      "(16110, 10) (16110, 2048)\n",
      "Tracking from frame 1 to 100\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 96 to 195\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 191 to 290\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 286 to 385\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 381 to 420\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Finished tracking, saving to output_visdrone_v2_e20/uav0000306_00230_v-GT.txt\n",
      "Sequence uav0000355_00001_v, GT detection, app thresh 0.75, dist thresh 100, retain length 3\n",
      "(18871, 10) (18871, 2048)\n",
      "Tracking from frame 1 to 100\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 96 to 195\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 191 to 290\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 286 to 385\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 381 to 468\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Finished tracking, saving to output_visdrone_v2_e20/uav0000355_00001_v-GT.txt\n",
      "Sequence uav0000370_00001_v, GT detection, app thresh 0.75, dist thresh 100, retain length 3\n",
      "(3017, 10) (3017, 2048)\n",
      "Tracking from frame 1 to 100\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 96 to 195\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Tracking from frame 191 to 265\n",
      "0-th iteration\n",
      "1-th iteration\n",
      "Finished tracking, saving to output_visdrone_v2_e20/uav0000370_00001_v-GT.txt\n"
     ]
    }
   ],
   "source": [
    "app_thresh = 0.75 #0.7, 0.8\n",
    "nms_thresh, eps = 0.3, 1e-7\n",
    "\n",
    "# for seq in ['MOT17-01', 'MOT17-03', 'MOT17-06', 'MOT17-07', 'MOT17-08', 'MOT17-12', 'MOT17-14']:\n",
    "# for seq in ['uav0000086_00000_v', 'uav0000117_02622_v', 'uav0000137_00458_v', 'uav0000182_00000_v', 'uav0000268_05773_v', 'uav0000305_00000_v', 'uav0000339_00001_v']:\n",
    "\n",
    "for seq in test_seq:\n",
    "    img = Image.open(f\"/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test-dev/sequences/{seq}/0000001.jpg\").convert('RGB')\n",
    "    if seq == \"uav0000073_04464_v\" or seq == \"uav0000161_00000_v\":\n",
    "        continue\n",
    "    # img =  Image.open(f\"/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-val/sequences/{seq}/0000001.jpg\").convert('RGB')\n",
    "    img_Height, img_Width = img.size\n",
    "\n",
    "    #Static camera, moving camera\n",
    "    if seq in ['MOT17-03']:\n",
    "        batch_size, dist_thresh, prune_len = 50, 50, 2 #tracklets less than 2 are pruned\n",
    "    else:\n",
    "        batch_size, dist_thresh, prune_len = 100, 100, 3\n",
    "        \n",
    "    # if seq == 'MOT17-06':\n",
    "    #     img_Height, img_Width = 480, 640\n",
    "    # else:\n",
    "    #     img_Height, img_Width = 1080, 1920\n",
    "        \n",
    "    #for detector in ['DPM','FRCNN','SDP']:\n",
    "    for detector in ['GT']:\n",
    "        print('Sequence {}, {} detection, app thresh {}, dist thresh {}, retain length {}'.format(\n",
    "            seq, detector, app_thresh, dist_thresh, prune_len))\n",
    "        # if os.path.isfile('output_visdroneV2/{}-{}.txt'.format(seq, detector)):\n",
    "        #     continue\n",
    "        # det_file = f\"VisDrone/VisDrone2019-MOT-test-dev/det/{seq}.txt\"\n",
    "        # app_file = f\"VisDrone/VisDrone2019-MOT-test-dev/feature/{seq}\"\n",
    "\n",
    "        det_file = f\"VisDrone/VisDrone2019-MOT-test-dev/annotations/{seq}.txt\"\n",
    "        app_file = f\"VisDrone/VisDrone2019-MOT-test-dev/feature_gt_box/{seq}\"\n",
    "\n",
    "        # det_file = './result/{}/det_{}.txt'.format(seq, detector)\n",
    "        # app_file = './result/{}/app_det_{}.npy'.format(seq, detector)\n",
    "\n",
    "        dets = np.loadtxt(det_file, delimiter=',')\n",
    "        app_feats = np.load(app_file)\n",
    "        print(dets.shape, app_feats.shape)\n",
    "        assert dets.shape[0] == app_feats.shape[0], 'Shape mismatch'\n",
    "\n",
    "        batch_overlap = 5                  #Number of frames to overlap between 2 batches\n",
    "        num_frames = int(dets[:, 0].max()) #Number of frames for this video\n",
    "        tracks_list, assignments_list, features_list, nms_list = [],[],[],[]\n",
    "        \n",
    "        for start_frame in range(1, num_frames+1, batch_size-batch_overlap):\n",
    "            end_frame = start_frame + batch_size - 1\n",
    "            if end_frame >= num_frames:\n",
    "                end_frame = num_frames\n",
    "                \n",
    "            print('Tracking from frame %d to %d'%(start_frame, end_frame))\n",
    "            curr_ind = np.logical_and(dets[:, 0] >= start_frame, dets[:, 0] <= end_frame)\n",
    "            curr_dets = np.concatenate([dets[curr_ind, 0][:, None], dets[curr_ind, 2:7],\n",
    "                                        np.arange(dets[curr_ind].shape[0])[:, None]], axis=1)\n",
    "\n",
    "            curr_dets[:, 3:5] = curr_dets[:, 3:5] + curr_dets[:, 1:3] # convert to frame,x1,y1,x2,y2,conf,node_ind\n",
    "            curr_app_feats = app_feats[curr_ind]\n",
    "            curr_app_feats = curr_app_feats / np.linalg.norm(curr_app_feats, axis=1, keepdims=True)\n",
    "            for iteration in range(2):\n",
    "                if iteration == 0:\n",
    "                    print('%d-th iteration'%iteration)\n",
    "                    linkIndexGraph, probs = get_trans_probs(tracker, curr_dets, curr_app_feats, \n",
    "                                                            app_thresh, max_frame_gap = 5)\n",
    "                    trans_cost = - np.log(probs + eps) #np.log((1 - probs + eps)/(probs + eps))\n",
    "                    det_cost = - curr_dets[:, -2]\n",
    "                    entry_cost = 0.5 * np.ones(det_cost.shape[0])\n",
    "                    exit_cost = entry_cost\n",
    "                    cost = np.concatenate((det_cost, entry_cost, exit_cost, trans_cost))\n",
    "\n",
    "                    A_eq, b_eq, A_ub, b_ub = tracker.build_constraint(linkIndexGraph)\n",
    "                    sol = tracker.linprog(c=cost, A_eq=A_eq, b_eq=b_eq, A_ub=A_ub, b_ub=b_ub)\n",
    "                    \n",
    "                    tracklets = tracker.recoverTracklets(curr_dets, sol, linkIndexGraph, prune_len=prune_len)    \n",
    "                    tracklets_ = np.delete(tracklets, -1, axis=1)\n",
    "                    interpolated_tracklets = interpolateTracks(tracklets_)\n",
    "                    \n",
    "                else:\n",
    "                    print('%d-th iteration'%iteration)\n",
    "                    assignment_list, feature_list = tracker.clusterSkipTracklets(tracklets, curr_app_feats, \n",
    "                                                                                 dist_thresh, app_thresh)\n",
    "                    tracks = tracker.recoverClusteredTracklets(tracklets, assignment_list)\n",
    "                    tracks = interpolateTracks(tracks)\n",
    "\n",
    "                    assignments_list.append(assignment_list)\n",
    "                    feature_array = np.stack(feature_list)\n",
    "                    feature_array = feature_array / np.linalg.norm(feature_array, axis=1, keepdims=True)\n",
    "                    \n",
    "            tracks_list.append(tracks)\n",
    "            features_list.append(feature_array)\n",
    "            \n",
    "        final_tracks = tracker.stitchTracklets(tracks_list, features_list)\n",
    "        final_tracks = remove_duplicates(final_tracks)\n",
    "        save_file = 'output_visdrone_v2_e20/{}-{}.txt'.format(seq, detector)\n",
    "        print('Finished tracking, saving to {}'.format(save_file))\n",
    "        np.savetxt(save_file, final_tracks, fmt='%d',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show final tracking and detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = [\"uav0000009_03358_v\",\n",
    "\"uav0000073_00600_v\",\n",
    "\"uav0000077_00720_v\",\n",
    "\"uav0000088_00290_v\",\n",
    "\"uav0000119_02301_v\",\n",
    "\"uav0000120_04775_v\",\n",
    "\"uav0000188_00000_v\",\n",
    "\"uav0000201_00000_v\",\n",
    "\"uav0000249_00001_v\",\n",
    "\"uav0000249_02688_v\",\n",
    "\"uav0000297_00000_v\",\n",
    "\"uav0000297_02761_v\",\n",
    "\"uav0000306_00230_v\",\n",
    "\"uav0000355_00001_v\",\n",
    "\"uav0000370_00001_v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uav0000009_03358_v\n",
      "save dir visualization/uav0000009_03358_v-GT\n",
      "Processing frame 100\n",
      "Processing frame 200\n",
      "Video saved at visualization/uav0000009_03358_v-GT.mp4\n",
      "uav0000073_00600_v\n",
      "save dir visualization/uav0000073_00600_v-GT\n",
      "Processing frame 100\n",
      "Processing frame 200\n",
      "Processing frame 300\n",
      "Video saved at visualization/uav0000073_00600_v-GT.mp4\n",
      "uav0000077_00720_v\n",
      "save dir visualization/uav0000077_00720_v-GT\n",
      "Processing frame 100\n",
      "Processing frame 200\n",
      "Processing frame 300\n",
      "Processing frame 400\n",
      "Processing frame 500\n",
      "Processing frame 600\n",
      "Processing frame 700\n",
      "Video saved at visualization/uav0000077_00720_v-GT.mp4\n",
      "uav0000088_00290_v\n",
      "save dir visualization/uav0000088_00290_v-GT\n",
      "Processing frame 100\n",
      "Processing frame 200\n",
      "Video saved at visualization/uav0000088_00290_v-GT.mp4\n",
      "uav0000119_02301_v\n",
      "save dir visualization/uav0000119_02301_v-GT\n",
      "Processing frame 100\n",
      "Video saved at visualization/uav0000119_02301_v-GT.mp4\n",
      "uav0000120_04775_v\n",
      "save dir visualization/uav0000120_04775_v-GT\n",
      "Processing frame 100\n",
      "Processing frame 200\n",
      "Processing frame 300\n",
      "Processing frame 400\n",
      "Processing frame 500\n",
      "Processing frame 600\n",
      "Processing frame 700\n",
      "Processing frame 800\n",
      "Processing frame 900\n",
      "Processing frame 1000\n",
      "Video saved at visualization/uav0000120_04775_v-GT.mp4\n",
      "uav0000188_00000_v\n",
      "save dir visualization/uav0000188_00000_v-GT\n",
      "Processing frame 100\n",
      "Processing frame 200\n",
      "Video saved at visualization/uav0000188_00000_v-GT.mp4\n",
      "uav0000201_00000_v\n",
      "save dir visualization/uav0000201_00000_v-GT\n",
      "Processing frame 100\n",
      "Processing frame 200\n",
      "Processing frame 300\n",
      "Processing frame 400\n",
      "Processing frame 500\n",
      "Processing frame 600\n",
      "Video saved at visualization/uav0000201_00000_v-GT.mp4\n",
      "uav0000249_00001_v\n",
      "save dir visualization/uav0000249_00001_v-GT\n",
      "Processing frame 100\n",
      "Processing frame 200\n",
      "Processing frame 300\n",
      "Video saved at visualization/uav0000249_00001_v-GT.mp4\n",
      "uav0000249_02688_v\n",
      "save dir visualization/uav0000249_02688_v-GT\n",
      "Processing frame 100\n",
      "Processing frame 200\n",
      "Video saved at visualization/uav0000249_02688_v-GT.mp4\n",
      "uav0000297_00000_v\n",
      "save dir visualization/uav0000297_00000_v-GT\n",
      "Processing frame 100\n",
      "Video saved at visualization/uav0000297_00000_v-GT.mp4\n",
      "uav0000297_02761_v\n",
      "save dir visualization/uav0000297_02761_v-GT\n",
      "Processing frame 100\n",
      "Processing frame 200\n",
      "Processing frame 300\n",
      "Video saved at visualization/uav0000297_02761_v-GT.mp4\n",
      "uav0000306_00230_v\n",
      "save dir visualization/uav0000306_00230_v-GT\n",
      "Processing frame 100\n",
      "Processing frame 200\n",
      "Processing frame 300\n",
      "Processing frame 400\n",
      "Video saved at visualization/uav0000306_00230_v-GT.mp4\n",
      "uav0000355_00001_v\n",
      "save dir visualization/uav0000355_00001_v-GT\n",
      "Processing frame 100\n",
      "Processing frame 200\n",
      "Processing frame 300\n",
      "Processing frame 400\n",
      "Video saved at visualization/uav0000355_00001_v-GT.mp4\n",
      "uav0000370_00001_v\n",
      "save dir visualization/uav0000370_00001_v-GT\n",
      "Processing frame 100\n",
      "Processing frame 200\n",
      "Video saved at visualization/uav0000370_00001_v-GT.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#seq = 'MOT17-03'\n",
    "detector = 'GT'\n",
    "# ['uav0000013_00000_v', 'uav0000013_01073_v', 'uav0000013_01392_v', 'uav0000020_00406_v', 'uav0000071_03240_v', 'uav0000072_04488_v']\n",
    "# for seq in ['MOT17-01', 'MOT17-03', 'MOT17-06', 'MOT17-07', 'MOT17-08', 'MOT17-12', 'MOT17-14']:\n",
    "# for seq in os.listdir(\"/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-val/sequences/\"):\n",
    "for seq in test_seq:\n",
    "    print(seq)\n",
    "    save_dir = 'visualization/{}-{}'.format(seq, detector)\n",
    "    print('save dir {}'.format(save_dir))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    #tracks: frame, ID, x, y, w, h, -1, -1, -1, -1\n",
    "    #dets:   frame, -1, x, y, w, h, conf, -1, -1, -1\n",
    "    # tracks = np.loadtxt(f'BYTE_Results/{seq}-{detector}.txt', delimiter=',')\n",
    "    # tracks = np.loadtxt(f'output_visdrone/{seq}-{detector}.txt', delimiter=',')\n",
    "    # tracks = np.loadtxt(f'VisDrone/VisDrone2019-MOT-test-dev/det/{seq}.txt', delimiter=',')\n",
    "    tracks = np.loadtxt(f'output_visdroneV2/{seq}-{detector}.txt', delimiter=',')\n",
    "        \n",
    "    tracks = tracks.astype(np.int32)\n",
    "    \n",
    "    colors = np.random.rand(1000,3)\n",
    "    resize_scale = 0.5\n",
    "    \n",
    "    # Get the first frame to determine video dimensions\n",
    "    first_frame = tracks[:, 0].min()\n",
    "    # img_file = os.path.join('data/MOT/MOT17/test/{}-{}/img1/{:06d}.jpg'.format(seq, detector, first_frame))\n",
    "    img_file = os.path.join('/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test-dev/sequences/{}/{:07d}.jpg').format(seq, first_frame)\n",
    "    img = cv2.imread(img_file)\n",
    "    height, width = int(resize_scale*img.shape[0]), int(resize_scale*img.shape[1])\n",
    "    \n",
    "    # Initialize video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # MP4 codec\n",
    "    video_path = os.path.join(\"visualization\", '{}-{}.mp4'.format(seq, detector))\n",
    "    fps = 30  # Frames per second for the output video\n",
    "    video_writer = cv2.VideoWriter(video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    for frame in range(tracks[:, 0].min(), tracks[:, 0].max()+1):\n",
    "        if frame % 100 == 0:\n",
    "            print('Processing frame {}'.format(frame))\n",
    "        \n",
    "        img_file = os.path.join('/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test-dev/sequences/{}/{:07d}.jpg').format(seq, frame)\n",
    "        img = cv2.imread(img_file)\n",
    "        img = cv2.resize(img, (int(resize_scale*img.shape[1]), int(resize_scale*img.shape[0])))\n",
    "        cv2.putText(img, '{:04}'.format(frame), (0,50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,0,255), thickness=2)\n",
    "        bboxes = tracks[tracks[:, 0] == frame, 1:6]\n",
    "        \n",
    "        if bboxes.shape[0] != 0:\n",
    "            #detections = dets[dets[:, 0] == frame, 2:7]\n",
    "            for i in range(bboxes.shape[0]):\n",
    "                ID = int(bboxes[i][0])\n",
    "                x, y = int(resize_scale*(bboxes[i][1])), int(resize_scale*(bboxes[i][2]))\n",
    "                w, h = int(resize_scale*(bboxes[i][3])), int(resize_scale*(bboxes[i][4]))\n",
    "                cv2.rectangle(img, (x,y), (x+w,y+h), 255*colors[ID], thickness=2)\n",
    "                cv2.putText(img, str(ID), (x,y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, 255*colors[ID], thickness=2)\n",
    "        \n",
    "        # Write frame to both image file and video\n",
    "        cv2.imwrite(save_dir+'/'+'{:06d}.jpg'.format(frame), img)\n",
    "        video_writer.write(img)\n",
    "    \n",
    "    # Release the video writer\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved at {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"BYTE_Results/MOT17-01-DPM/MOT17-01-DPM.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"BYTE_Results/MOT17-01-DPM/MOT17-01-DPM.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import generalized_box_iou\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khanh/miniconda3/envs/LPT/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/khanh/miniconda3/envs/LPT/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Load pretrained ResNet50 model without the final classification layer\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "# 2. Define image transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 3. Custom dataset to apply transforms\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, transform):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.images[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [13:28<00:00, 47.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# for data_name in tqdm(os.listdir(\"/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test/sequences/\")):\n",
    "for data_name in tqdm(test_seq):\n",
    "    label_path = f\"/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test-dev/annotations/{data_name}.txt\"\n",
    "    # label_path = f\"/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test-dev/det/{data_name}.txt\"\n",
    "    video_path = f\"/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test-dev/sequences/{data_name}/\"\n",
    "    frame_names = os.listdir(video_path)\n",
    "    frame_names.sort()\n",
    "    frame_path_list = [os.path.join(video_path, frame_names[i]) for i in range(len(frame_names))]\n",
    "    # <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <x>, <y>, <z>\n",
    "    gt = []\n",
    "    with open(label_path, 'r') as f:\n",
    "        for l in f.readlines():\n",
    "            l = l.strip().split(',')\n",
    "            gt.append([int(i) for i in l[:-1]] + [float(l[-1])])\n",
    "    gt = np.array(gt)\n",
    "    gt[:,4:6] = gt[:,2:4] + gt[:,4:6]\n",
    "    \n",
    "    video_frames = [None]\n",
    "    for path in frame_path_list:\n",
    "        video_frames.append(Image.open(path).convert('RGB'))\n",
    "    \n",
    "    isinstance_PIL = []\n",
    "    for instance in gt:\n",
    "        isinstance_PIL.append(video_frames[int(instance[0])].crop(instance[2:6]))\n",
    "    \n",
    "    instance_embedding = []\n",
    "    dataset = ImageDataset(isinstance_PIL, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            output = resnet(batch).squeeze(-1).squeeze(-1)  # Shape: (B, 2048)\n",
    "            embeddings = output.cpu()  # Move to CPU if needed\n",
    "            instance_embedding.extend(embeddings)\n",
    "    instance_embedding = torch.stack(instance_embedding)\n",
    "    \n",
    "    with open(f'/home/khanh/data/LPT/VisDrone/VisDrone2019-MOT-test-dev/feature_gt_box/{data_name}', 'wb') as f:\n",
    "        np.save(f, instance_embedding.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3610923/822068899.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load('ckpt/visdrone/epoch-10.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load('ckpt/visdrone/epoch-10.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_file = f'data/MOT16/train/MOT16-09/det/det.txt'\n",
    "app_file = f'data/MOT/MOT17/train/{seq}-{detector}/feature.npy'\n",
    "\n",
    "dets = np.loadtxt(det_file, delimiter=',')\n",
    "app_feats = np.load(app_file)\n",
    "assert dets.shape[0] == app_feats.shape[0], 'Shape mismatch'\n",
    "\n",
    "batch_overlap = 5                  #Number of frames to overlap between 2 batches\n",
    "num_frames = int(dets[:, 0].max()) #Number of frames for this video\n",
    "tracks_list, assignments_list, features_list, nms_list = [],[],[],[]\n",
    "\n",
    "for start_frame in range(1, num_frames+1, batch_size-batch_overlap):\n",
    "    end_frame = start_frame + batch_size - 1\n",
    "    if end_frame >= num_frames:\n",
    "        end_frame = num_frames\n",
    "        \n",
    "    print('Tracking from frame %d to %d'%(start_frame, end_frame))\n",
    "    curr_ind = np.logical_and(dets[:, 0] >= start_frame, dets[:, 0] <= end_frame)\n",
    "    curr_dets = np.concatenate([dets[curr_ind, 0][:, None], dets[curr_ind, 2:7],\n",
    "                                np.arange(dets[curr_ind].shape[0])[:, None]], axis=1)\n",
    "\n",
    "    curr_dets[:, 3:5] = curr_dets[:, 3:5] + curr_dets[:, 1:3] # convert to frame,x1,y1,x2,y2,conf,node_ind\n",
    "    curr_app_feats = app_feats[curr_ind]\n",
    "    curr_app_feats = curr_app_feats / np.linalg.norm(curr_app_feats, axis=1, keepdims=True)\n",
    "    for iteration in range(2):\n",
    "        if iteration == 0:\n",
    "            print('%d-th iteration'%iteration)\n",
    "            linkIndexGraph, probs = get_trans_probs(tracker, curr_dets, curr_app_feats, \n",
    "                                                    app_thresh, max_frame_gap = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=6, out_features=6, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=6, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LPT",
   "language": "python",
   "name": "lpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
